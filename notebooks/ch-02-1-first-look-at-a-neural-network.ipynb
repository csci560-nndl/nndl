{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e535a40b-a002-46cb-b656-a92ad4ddbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress tensorflow logging, usually not useful unless you are having problems with tensorflow or accessing gpu\n",
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "# globally useful libraries / namespaces for this notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook wide settings for plotting visualizations\n",
    "plt.rcParams['figure.figsize'] = (10, 8) # set default figure size, 10in by 8in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b2548-990e-4146-93e5-5ef3d2e77760",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 2: The Mathematical Building Blocks of Neural Networks\n",
    "\n",
    "Supporting materials for:\n",
    "\n",
    "Chollet (2021). *Deep Learning with Python*. 2nd ed. Manning Publications Co.\n",
    "[Amazon](https://www.amazon.com/Learning-Python-Second-Fran%C3%A7ois-Chollet/dp/1617296864/ref=sr_1_1?crid=32NFM2SBCJVQQ)\n",
    "\n",
    "Understanding deep learning requires familiarity with many simple mathematical\n",
    "concepts: tensors, tensor operations, differentiation, gradient descent, and so on.\n",
    "Out goal in this notebook/chapter is to build up your intiution about these mathematical\n",
    "concepts without getting over technical.\n",
    "\n",
    "One note about the term tensors.  **Tensor** is a mathematical term that simply can be used to refer\n",
    "to a matrix of values, no matter how many dimensions.  Thus a 1-dimensional vector or a 2-dimensional\n",
    "matrix are both *Tensors*, as well as 3-dimensional or higher matrices.  Vectorized programming\n",
    "could also be referred to as tensor programming, but the term vectorized programming was coined\n",
    "before the term tensor was in common use.  Another difference is that the term Tensor programming\n",
    "and Tensor libraries have come to mean the types of vectorized or tensor operations we usually think of, but\n",
    "it also includes libraries that can automatically differentiate such tensors with respect to\n",
    "a loss function.  We will get to these concepts in a bit more detail below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f34d6a-67f6-436c-8c79-f70e0546d9d9",
   "metadata": {},
   "source": [
    "## Note: Installing Keras/TensorFlow\n",
    "\n",
    "For this textbook we are using the `TensorFlow` library and mostly accessing the `Keras`\n",
    "neural network and deep learning API through the `TensorFlow`.  You will need to have\n",
    "a recent version of `Tensorflow/Keras` installed on your machine and available to your\n",
    "iPython kernel to run this notebook.  At the time of creation of this notebook, \n",
    "`TensorFlow/Keras` is no longer supported directly by the `Conda` package manager.  You\n",
    "should be doing a `pip install` command of `TensorFlow` to correctly get the \n",
    "most recent `TensorFlow` libraries with the `Keras` API available:\n",
    "\n",
    "```\n",
    "$ python3 -m pip install 'tensorflow'\n",
    "```\n",
    "\n",
    "If you have a suitable Nvidia GPU processor available on your your machine, you can get the `cuda` libraries\n",
    "setup and installed for `TensorFlow` by doing:\n",
    "\n",
    "```\n",
    "python3 -m pip install 'tensorflow[and-cuda]'\n",
    "```\n",
    "\n",
    "See the official [Install TensorFlow 2](https://www.tensorflow.org/install) for more detailed information about correctly\n",
    "getting `TensorFlow/Keras` installed on your system. [Install TensorFlow with pip](https://www.tensorflow.org/install/pip) \n",
    "has information on installing cuda/Nvidia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721427b-2041-41b4-a30a-4118333e28ee",
   "metadata": {},
   "source": [
    "# 2.1 A first look at a neural network\n",
    "\n",
    "The problem we will look at initially is building a neural network to classify handwritten digits.  \n",
    "The MNIST dataset (a classic in ML) contains grayscale images of handwritten digits\n",
    "that are 28 x 28 pixels in size, and each image is labeled with their correct category 0-9.\n",
    "\n",
    "The MNIST dataset contains 60,000 images meant to be used for training, and another 10,000\n",
    "images we can use for testing.  The following command loads the MNIST dataset.  This dataset has\n",
    "been referred to as the *fruit fly* model of vision machine learning, and is often used as a\n",
    "benchmark for doing neural network and deep learning examples.\n",
    "\n",
    "The dataset is still relatively big, despite being used for learning, so it actually downloads the\n",
    "data for you.  The result of loading the data is a set of `NumPy` n-dimensional arrays (Tensors)\n",
    "holding the images and the labeles of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9b3eef-e0b7-4558-ada4-77c13a2bb219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 18:51:08.808861: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747939868.828341   20310 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747939868.833946   20310 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747939868.847875   20310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747939868.847900   20310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747939868.847903   20310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747939868.847905   20310 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 18:51:08.852996: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# note: we will usually put all imports at top of notebook, unless\n",
    "# we are specifically learning about how to use a library\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346ebcc-d400-4851-a357-795eca8a6b1f",
   "metadata": {},
   "source": [
    "The images are encoded as `NumPy` arrays, and the labels are an array of digits, ranging\n",
    "from 0 to 9.  Here is the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dcbdb36-8844-4449-9aae-de9ca0d9887d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb5f16d-85e9-4b3e-ac45-1bed750113e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e011fb1-47c3-4c93-af46-b804a2bfaf3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa0b5b7e-196a-46c0-a15f-c042c95cab33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a9fa3a6-1e32-4de0-af74-92dbcf45a916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0104507b-fddf-4e7a-b73f-c415eeedbcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b3e66bb-a4ac-48de-8014-988456b4f9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7394f4b-768e-4a2c-9174-8f07894f3793",
   "metadata": {},
   "source": [
    "The test data is similar but there are only 10,000 different images in the testing dataset.\n",
    "\n",
    "The images are 28x28 shapes arrays of `uint8` greyscale pixels.  For example, the data for \n",
    "the first training image looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b72277d-a3ed-418c-a1bb-d9be1f492fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3a6ce0-d917-4c4c-b274-fa208177ab2d",
   "metadata": {},
   "source": [
    "But this is hard to visualize.  When can actually render the hand drawn digits to see what\n",
    "they look like as follows (You should try looking at some other images than image 0 we show here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "119f5561-cd79-4bd4-8d2b-de1b8f4f8413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for image:  3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make a slice of the 28x28 pixel values of training image 0\n",
    "img_no = 1234\n",
    "img = train_images[img_no,:,:]\n",
    "\n",
    "# display the imge using matplotlib\n",
    "plt.imshow(img, cmap='gray');\n",
    "\n",
    "# confirm that the label for this image matches\n",
    "print('Label for image: ', train_labels[img_no])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87d6d8-7e6c-4891-99ad-170c7eb86445",
   "metadata": {},
   "source": [
    "The workflow to create an image classifier using `TensorFlow/Keras` neural networks is\n",
    "\n",
    "1. First we'll create and compile a neural network architecture using the `Keras` library.\n",
    "2. We will need to do a little bit of data cleaning of the images to make them suitable for training with.\n",
    "3. Then we will feed the neural network the training data, `train_images` and\n",
    "   `train_labels`.  The network will learn to associate image inputs with the label as its output.\n",
    "4. Finally once the network is trained, we can ask the network to produce predictions for\n",
    "   the `test_images`, and we can evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d95298-dc52-436e-b91d-bc094c4009ff",
   "metadata": {},
   "source": [
    "## The network architecture\n",
    "\n",
    "The core building block of a neural network is the *layer*.  The following network architecutre is very simple.\n",
    "Don't worry about the details yet, we will learn about what all of this means as we go along in the course.\n",
    "\n",
    "A networks consists of a sequence of layers.  We will add 2 `Dense` layers to the network we are going to train.  A `Dense` layer\n",
    "is a fully connected layer, all inputs to the layer are fully connected to each output of the layer.\n",
    "\n",
    "We will talk about the activation functions in more detail later.\n",
    "Notice that the first layer uses an 'relu' activation\n",
    "function, while the second layer uses a 'softmax' activation function.\n",
    "A 10-way *softmax* layer means that this layer will return an array of 10 probability\n",
    "scores (summing to 1).  Each score will represent the probability that the current\n",
    "image belongs to one of the 10 output labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14b7091d-3e66-4b3f-a270-09a4d60e8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# create a full model with a dense layer and activation layer\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79065c63-3750-4b22-8c1e-f62925cee47d",
   "metadata": {},
   "source": [
    "To make the model ready for training, we need to specify three more things as part of the\n",
    "*compilation* step to finalize the network architecture:\n",
    "\n",
    "1. A *loss function* - How the network will measure its performance\n",
    "2. An *optimizer* - The mechanism through wich the network updates itself\n",
    "3. *Metrics to monitor* during training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "742c456e-5755-49eb-8c07-a201e6b8406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 18:51:14.261206: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f88ac-541f-44c5-a8b4-9d2b50f8c63a",
   "metadata": {},
   "source": [
    "Again don't be too worried yet about the details here, we will discuss all of these concepts in our\n",
    "class.  In this first example, our model will use `categorical_crossentropy` as the loss function,\n",
    "and the simple `rmsprop` optimizer to perform gradient descent on the network weights.  The library will\n",
    "track and report on the accuracy of the model as it trains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353418-ffa0-44d9-9f5b-859a564252ee",
   "metadata": {},
   "source": [
    "## Preparing the image data\n",
    "\n",
    "Before training, we need to do a little bit of preprocessing and data cleaning.  This is not unusual, you will\n",
    "often need to perform some data preparation on your data before it can be used successfully by a machine\n",
    "learning algorithm.\n",
    "\n",
    "We will reshape all of the image data, into a flat set of 28x28 = 784 inputs, which is what the input layer to\n",
    "this network expects.  \n",
    "\n",
    "Also before training we will scale all of the input to values in the `[0, 1]`.  If you noticed above, all\n",
    "of the image pixel values are currently in the range `[0, 255]`.  Previously our training images\n",
    "were stored in an array of shape `(60000, 28, 28)` of type `uint8`.  We will transform it into\n",
    "a `float32` array of shape `(60000, 28 * 28)` with all pixel values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96e19f2c-0227-43b8-b493-76ca09349dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "# flatten the 2-d images to 1-d of 784 inputs\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "# transform the greyscale integer values to floats for input\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "\n",
    "# show results\n",
    "print( train_images.shape )\n",
    "print( train_images.dtype )\n",
    "\n",
    "# transform the test images as well in the same way while we are at it\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7619f-7d59-405f-bcb4-84873741b369",
   "metadata": {},
   "source": [
    "**NOTE**: The following is not shown in my 2nd edition of the textbook.  However, the\n",
    "training labels must have the same shape as the output layer for a `Keras` network.  Currently\n",
    "the labels are simply values from `[0 - 9]` but instead we need an array of 10\n",
    "columns with a `1` for the image category, and `0` for all others.  We can use the \n",
    "`to_categorical()` method to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e52e7c4c-58c6-4c04-ac81-f7891b7ce2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# notice before the shape of the labels and their values\n",
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae4b0094-8063-4a95-8371-640e6bf24628",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# use 1-hot encoding to make into categorical labels\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435adb66-0a50-4254-9177-8a443e0ad7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# now the labels have 10 columns, with a 1 set for the image category\n",
    "print(train_labels.shape)\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fd448b-bd7e-43a4-a24b-879cb0622d3b",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We're now ready to train the model, which in the `Keras` API is done by calling the model's `fit()`\n",
    "method.  We *fit* the model to its training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e784c59e-b21c-4d63-a4b3-1e028e33e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8747 - loss: 0.4322\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9659 - loss: 0.1168\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.9780 - loss: 0.0724\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.9854 - loss: 0.0503\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9897 - loss: 0.0366\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e92a6-107d-43d3-884f-dc04ec4073f8",
   "metadata": {},
   "source": [
    "Notice that the measure of the loss and the accuracy are reported by the training here.\n",
    "We will discuss how a loss (or fitness) function works in more detail, it is being\n",
    "used to actually change the weights to improve the performance of the model.  So you should\n",
    "see the loss decrease over the 5 epochs of training since we are minimizing the loss\n",
    "here.\n",
    "\n",
    "Likewise notice that the acuracy is reported since we asked that metric to be monitored\n",
    "when we compiled the network architecture.  You should typically see that we get\n",
    "over 98% accuracy on the training data after 5 epochs of training.\n",
    "over 98% accuracy on the training data after only 5 epochs of training.\n",
    "\n",
    "But the true test is how well the trained network does on data it has not seen before.  We\n",
    "can evaluate the performance on the test data like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323b859-c26a-4a8f-8556-c33772d6de18",
   "metadata": {},
   "source": [
    "## Using the model to make predictions\n",
    "\n",
    "Now that we have trained a model, we can use it to make predictions on images that it has never seen before. \n",
    "For example, lets take the first test images and ask the model to make predictions of what\n",
    "digit category is in those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b54aba35-e2e1-4876-a2ce-72350d8cb79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.0132537e-08, 1.6266772e-09, 4.1710368e-06, 5.6054610e-05,\n",
       "       4.5472580e-12, 4.6540372e-08, 3.5877470e-13, 9.9993932e-01,\n",
       "       2.1280117e-08, 4.3399370e-07], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the astute student might notice we pull out two digits here, but we are only going to look at the index 0 image prediction\n",
    "test_digit = test_images[0:1]\n",
    "predictions = model.predict(test_digit)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bec94d-73de-4d5b-806c-01d38a5a344f",
   "metadata": {},
   "source": [
    "Don't let the scientific notation of the output confuse you.  If it is easier, we can ask `NumPy`\n",
    "to display the values using fixed notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39cf24d8-901d-4ea3-9f56-2fcc3e42bdd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000001, 0.        , 0.00000417, 0.00005605, 0.        ,\n",
       "       0.00000005, 0.        , 0.9999393 , 0.00000002, 0.00000043],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppress scientific notation and show 8 digits of precision\n",
    "np.set_printoptions(suppress=True, precision=8)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb6d583-5e6e-412a-af69-f9548738f89f",
   "metadata": {},
   "source": [
    "Most of the values were close to 0, except the value at position 7 of the array.  This means the\n",
    "fitted model is giving a 99% probability that the input image category is 7.  We can confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f050f8-cf8f-4ad5-8a25-c96ebc40c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for image:  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Use argmax to determine maximum index, which is the category: 7\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_no = 0\n",
    "img = test_digit[img_no,:].reshape(28,28)\n",
    "\n",
    "# display the imge using matplotlib\n",
    "plt.imshow(img, cmap='gray');\n",
    "\n",
    "# confirm that the label for this image matches\n",
    "print('Label for image: ', test_labels[img_no])\n",
    "print('Use argmax to determine maximum index, which is the category:', predictions[0].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff152ca-fbe6-49f3-ba0a-7da403f2598e",
   "metadata": {},
   "source": [
    "We saw that the accuracy when training the model was over 99% accurate.  But how well a model\n",
    "does on the data it is trained with is not always a good indication of how the model will do\n",
    "wit data it has never seen before.  Let's check the accuracy of the fitted model on the \n",
    "entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "562c9b30-592b-45d2-bc88-4cf0bafe883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9777 - loss: 0.0738\n",
      "test_loss:  0.06346900761127472\n",
      "test_acc:   0.9800999760627747\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_loss: ', test_loss)\n",
    "print('test_acc:  ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d65f3c-14ca-4722-8a8a-d91ba7139f89",
   "metadata": {},
   "source": [
    "You should usually see that the accuracy falls down to 97.8% here.  This may not seem to significant, but\n",
    "it is quite a bit lower than the 98.9% accuracy that the model gets on the data while training.  \n",
    "\n",
    "This gap between training and test accuracy is an example of *overfitting*.  Machine learning models tend to\n",
    "perform worse on new data than on the data they are trained with because they overlean and model some of\n",
    "the noise present in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
