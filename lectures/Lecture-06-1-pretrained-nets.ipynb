{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:40:03.685465Z",
     "start_time": "2020-03-24T19:40:03.007146Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:40:04.604715Z",
     "start_time": "2020-03-24T19:40:04.596679Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 8) # set default figure size, 10in by 8in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Introduction to Deep Learning for Computer Vision\n",
    "Supporting materials for:\n",
    "\n",
    "Chollet (2021). *Deep Learning with Python*. v2 Manning Publications Co. \n",
    "\n",
    "Chapter 8 *Introduction to Deep Learning for Computer Vision*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 Leveraging a pretrained model\n",
    "\n",
    "A common and highly effective approach for image classifiers\n",
    "is to use a *pretrained network*. For instance we might use\n",
    "the initial layers trained on the well ImageNet task\n",
    "(where classes are mostly animals and everyday objects).\n",
    "The basic idea is that for many image classification tasks,\n",
    "there are many common low level features that can be\n",
    "learned and recognized, and these features generalize\n",
    "to many different kinds of tasks.  \n",
    "\n",
    "## 8.3.1 Feature extraction with a pretrained model\n",
    "\n",
    "In our textbook and in this notebook, we will attempt to\n",
    "use the VGG16 architecture that has been trained\n",
    "on the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:40:15.166214Z",
     "start_time": "2020-03-24T19:40:08.016028Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VGG16 model, among others, is already prepackaged\n",
    "with Keras.  So we can import an already trained\n",
    "version of this model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:42:36.401980Z",
     "start_time": "2020-03-24T19:42:02.534029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 14:33:08.227774: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:08.240881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:08.241318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:08.242171: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-02 14:33:08.242849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:08.243303: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:08.243671: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:09.123074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:09.123415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:09.123659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-02 14:33:09.123871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 138 MB memory:  -> device: 0, name: Quadro M2200, pci bus id: 0000:01:00.0, compute capability: 5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58892288/58889256 [==============================] - 7s 0us/step\n",
      "58900480/58889256 [==============================] - 7s 0us/step\n"
     ]
    }
   ],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the textbook, the parameters used when\n",
    "loading the VGG16 convnet are\n",
    "- `weights` the specific weight checkpoint to use, e.g. use \n",
    "  the ImageNet trained weights checkpoint.\n",
    "- `include_top` whether to include (or not) the final\n",
    "  densely connected layers.  convnet typically have\n",
    "  a final fully connected or dense layer to perform\n",
    "  final classification, just as our example in the previous\n",
    "  notebook used.  We want to train our own fully connected\n",
    "  layers using the pretrained convolutional layers, thus we\n",
    "  do not wish to include the top layers of this network.\n",
    "- `input_shape` A nice feature of Keras pretrained networks\n",
    "  the library supports feeding in different sized images,\n",
    "  possibly different from the original training used for this\n",
    "  pretrained network.  This makes it relatively easy to pull\n",
    "  in a pretrained network for a new task and try it out on \n",
    "  your images.\n",
    "  \n",
    "As with all `keras` networks, we can get a summary of the\n",
    "network to see the details of the architecture of our\n",
    "pretrained network.  It is similar to the\n",
    "convnet we looked at previously, with alternating\n",
    "convolutional layers and max pooling layers, though\n",
    "in VGG16 there are 2 or 3 successive convolution layers\n",
    "before a max pooling layer.  Notice also the\n",
    "total number of parameters in the network, over 14\n",
    "million.  However we are not going to be training any of\n",
    "these weights, we will fix them and only train weigths\n",
    "on new densly connected layers we add to the top of\n",
    "this pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:42:46.215671Z",
     "start_time": "2020-03-24T19:42:46.182531Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final max pooling layer has a feature map shape of\n",
    "`(5, 5, 512)`.  These are the features we will\n",
    "stick a densly connected classifier onto to train with\n",
    "our cats vs. dogs image classification dataset.\n",
    "\n",
    "There is a technical decision that can be made at this point.\n",
    "There are two common ways to train the fully connected\n",
    "layers we want to add to the network for our task.\n",
    "\n",
    "1. Since the weights of the base will not change, we could\n",
    "simply run all of the training and test images we have\n",
    "through the base fixed network, record the output\n",
    "tensor feature maps for each, then use these as the inputs\n",
    "for training a new standalone densely connected neural\n",
    "network classifier.  This is fast and cheap, we only need\n",
    "to run each image 1 time through the pretrained network.\n",
    "But if you think about it, this approach cannot be used\n",
    "together with data augmentation, as there are an infinite\n",
    "number of augmented images we can generate.\n",
    "2. We can extend the model we have (the conv_base) by adding\n",
    "on the `Dense` layers we want to the top, fix the weights\n",
    "of the base convolutional layers, and then use streamed\n",
    "training as we did previously on either the fixed images\n",
    "or on augmented image input streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast feature extraction without data augmentation\n",
    "\n",
    "We will demonstrate the first method first.  We use the\n",
    "previous model where we create a `keras` `ImageGenerator`\n",
    "to stream images into the `conv_base`.  But we simply stream\n",
    "each image 1 time into the base network and record\n",
    "the final output tensor into a `Numpy` array along with\n",
    "the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience method to convert/process train, validation and test datasets as needed\n",
    "def get_features_and_labels(dataset, num_imgs):\n",
    "    \"\"\"Given a dataset (iterator), preprocess all images using VGG16 features\n",
    "    in the dataset and return the preprocessed image features.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    img_index = 0\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "        img_index += 1\n",
    "        if img_index == num_imgs:\n",
    "            break\n",
    "            \n",
    "    # concatenate all into a numpy array and return\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# need to reload the dataset iterators from previous notebook\n",
    "import os, shutil, pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "new_base_dir = pathlib.Path('../data/cats_and_dogs_small')\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is being run on GPU and is not fitting on my GPU memory at the moment\n",
    "with tf.device('/CPU:0'):\n",
    "    train_features, train_labels = get_features_and_labels(train_dataset, 2000)\n",
    "    val_features, val_labels = get_features_and_labels(validation_dataset, 1000)\n",
    "    test_features, test_labels = get_features_and_labels(test_dataset, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:45:33.265671Z",
     "start_time": "2020-03-24T19:43:07.373963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 5, 5, 512)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted features are currently of shape\n",
    "`(samples, 5, 5, 512)`.  We will feed them to a densely\n",
    "connected classifier, so we must first flatten\n",
    "them to a shape of `(samples, 12800)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can define our densely connected classifer\n",
    "(note the use of dropout for regularization) and train\n",
    "it on the data and labels that we just recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-02 16:04:57.949792: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 12.50MiB (rounded to 13107200)requested by op RandomUniform\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-03-02 16:04:57.949824: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2022-03-02 16:04:57.949837: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 71, Chunks in use: 71. 17.8KiB allocated for chunks. 17.8KiB in use in bin. 989B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949846: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 2, Chunks in use: 2. 1.0KiB allocated for chunks. 1.0KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949855: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 4.2KiB allocated for chunks. 4.2KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949865: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 6, Chunks in use: 6. 13.0KiB allocated for chunks. 13.0KiB in use in bin. 12.0KiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949873: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 1, Chunks in use: 1. 6.8KiB allocated for chunks. 6.8KiB in use in bin. 6.8KiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949882: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949890: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949898: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949906: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949915: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949926: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 2, Chunks in use: 2. 705.0KiB allocated for chunks. 705.0KiB in use in bin. 432.0KiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949937: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 1, Chunks in use: 1. 576.0KiB allocated for chunks. 576.0KiB in use in bin. 576.0KiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949948: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 1, Chunks in use: 1. 1.97MiB allocated for chunks. 1.97MiB in use in bin. 1.12MiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949956: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 4.50MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949962: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 2, Chunks in use: 1. 10.68MiB allocated for chunks. 4.50MiB in use in bin. 4.50MiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949969: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 11, Chunks in use: 11. 119.93MiB allocated for chunks. 119.93MiB in use in bin. 116.19MiB client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949975: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949980: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949985: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.949996: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.950001: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-03-02 16:04:57.950007: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 12.50MiB was 8.00MiB, Chunk State: \n",
      "2022-03-02 16:04:57.950012: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 145096704\n",
      "2022-03-02 16:04:57.950020: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0000 of size 1280 next 1\n",
      "2022-03-02 16:04:57.950025: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0500 of size 256 next 2\n",
      "2022-03-02 16:04:57.950031: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0600 of size 256 next 3\n",
      "2022-03-02 16:04:57.950038: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0700 of size 256 next 4\n",
      "2022-03-02 16:04:57.950043: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0800 of size 256 next 5\n",
      "2022-03-02 16:04:57.950048: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0900 of size 256 next 8\n",
      "2022-03-02 16:04:57.950052: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0a00 of size 256 next 9\n",
      "2022-03-02 16:04:57.950056: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0b00 of size 256 next 10\n",
      "2022-03-02 16:04:57.950061: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0c00 of size 256 next 13\n",
      "2022-03-02 16:04:57.950065: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0d00 of size 256 next 14\n",
      "2022-03-02 16:04:57.950070: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e0e00 of size 512 next 17\n",
      "2022-03-02 16:04:57.950074: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1000 of size 256 next 18\n",
      "2022-03-02 16:04:57.950079: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1100 of size 256 next 19\n",
      "2022-03-02 16:04:57.950083: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1200 of size 256 next 49\n",
      "2022-03-02 16:04:57.950087: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1300 of size 256 next 20\n",
      "2022-03-02 16:04:57.950091: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1400 of size 256 next 23\n",
      "2022-03-02 16:04:57.950096: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1500 of size 256 next 24\n",
      "2022-03-02 16:04:57.950100: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1600 of size 1024 next 27\n",
      "2022-03-02 16:04:57.950105: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1a00 of size 256 next 28\n",
      "2022-03-02 16:04:57.950110: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1b00 of size 256 next 29\n",
      "2022-03-02 16:04:57.950115: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e1c00 of size 1024 next 30\n",
      "2022-03-02 16:04:57.950123: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2000 of size 256 next 51\n",
      "2022-03-02 16:04:57.950131: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2100 of size 256 next 22\n",
      "2022-03-02 16:04:57.950138: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2200 of size 256 next 55\n",
      "2022-03-02 16:04:57.950145: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2300 of size 256 next 33\n",
      "2022-03-02 16:04:57.950152: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2400 of size 256 next 35\n",
      "2022-03-02 16:04:57.950160: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2500 of size 256 next 36\n",
      "2022-03-02 16:04:57.950167: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2600 of size 2048 next 39\n",
      "2022-03-02 16:04:57.950174: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2e00 of size 256 next 40\n",
      "2022-03-02 16:04:57.950182: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e2f00 of size 256 next 41\n",
      "2022-03-02 16:04:57.950189: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3000 of size 256 next 63\n",
      "2022-03-02 16:04:57.950197: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3100 of size 256 next 64\n",
      "2022-03-02 16:04:57.950204: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3200 of size 256 next 68\n",
      "2022-03-02 16:04:57.950212: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3300 of size 256 next 69\n",
      "2022-03-02 16:04:57.950219: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3400 of size 256 next 70\n",
      "2022-03-02 16:04:57.950226: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3500 of size 256 next 71\n",
      "2022-03-02 16:04:57.950233: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3600 of size 256 next 72\n",
      "2022-03-02 16:04:57.950241: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3700 of size 256 next 73\n",
      "2022-03-02 16:04:57.950248: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3800 of size 256 next 74\n",
      "2022-03-02 16:04:57.950256: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3900 of size 256 next 75\n",
      "2022-03-02 16:04:57.950263: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3a00 of size 256 next 77\n",
      "2022-03-02 16:04:57.950271: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3b00 of size 256 next 76\n",
      "2022-03-02 16:04:57.950278: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3c00 of size 256 next 78\n",
      "2022-03-02 16:04:57.950286: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3d00 of size 256 next 6\n",
      "2022-03-02 16:04:57.950293: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3e00 of size 256 next 54\n",
      "2022-03-02 16:04:57.950301: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e3f00 of size 512 next 15\n",
      "2022-03-02 16:04:57.950309: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e4100 of size 1024 next 26\n",
      "2022-03-02 16:04:57.950317: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e4500 of size 2048 next 37\n",
      "2022-03-02 16:04:57.950324: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e4d00 of size 3072 next 7\n",
      "2022-03-02 16:04:57.950332: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e5900 of size 2048 next 42\n",
      "2022-03-02 16:04:57.950339: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e6100 of size 2048 next 46\n",
      "2022-03-02 16:04:57.950347: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e6900 of size 2048 next 48\n",
      "2022-03-02 16:04:57.950354: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7100 of size 256 next 56\n",
      "2022-03-02 16:04:57.950362: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7200 of size 256 next 57\n",
      "2022-03-02 16:04:57.950369: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7300 of size 256 next 58\n",
      "2022-03-02 16:04:57.950377: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7400 of size 256 next 59\n",
      "2022-03-02 16:04:57.950384: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7500 of size 256 next 61\n",
      "2022-03-02 16:04:57.950392: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7600 of size 256 next 60\n",
      "2022-03-02 16:04:57.950400: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7700 of size 256 next 62\n",
      "2022-03-02 16:04:57.950408: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7800 of size 256 next 50\n",
      "2022-03-02 16:04:57.950416: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7900 of size 256 next 52\n",
      "2022-03-02 16:04:57.950424: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e7a00 of size 6912 next 53\n",
      "2022-03-02 16:04:57.950434: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13029e9500 of size 279552 next 12\n",
      "2022-03-02 16:04:57.950441: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1302a2d900 of size 442368 next 16\n",
      "2022-03-02 16:04:57.950449: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1302a99900 of size 2064384 next 21\n",
      "2022-03-02 16:04:57.950457: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1302c91900 of size 589824 next 11\n",
      "2022-03-02 16:04:57.950464: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1302d21900 of size 4718592 next 31\n",
      "2022-03-02 16:04:57.950472: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1900 of size 256 next 79\n",
      "2022-03-02 16:04:57.950480: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1a00 of size 256 next 80\n",
      "2022-03-02 16:04:57.950487: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1b00 of size 256 next 81\n",
      "2022-03-02 16:04:57.950494: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1c00 of size 256 next 82\n",
      "2022-03-02 16:04:57.950502: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1d00 of size 256 next 83\n",
      "2022-03-02 16:04:57.950509: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1e00 of size 256 next 84\n",
      "2022-03-02 16:04:57.950517: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a1f00 of size 256 next 85\n",
      "2022-03-02 16:04:57.950524: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2000 of size 256 next 88\n",
      "2022-03-02 16:04:57.950533: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2100 of size 256 next 89\n",
      "2022-03-02 16:04:57.950540: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2200 of size 256 next 90\n",
      "2022-03-02 16:04:57.950548: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2300 of size 256 next 91\n",
      "2022-03-02 16:04:57.950555: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2400 of size 256 next 92\n",
      "2022-03-02 16:04:57.950562: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2500 of size 256 next 93\n",
      "2022-03-02 16:04:57.950570: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2600 of size 256 next 94\n",
      "2022-03-02 16:04:57.950577: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2700 of size 256 next 95\n",
      "2022-03-02 16:04:57.950585: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2800 of size 256 next 96\n",
      "2022-03-02 16:04:57.950592: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2900 of size 256 next 97\n",
      "2022-03-02 16:04:57.950600: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2a00 of size 256 next 98\n",
      "2022-03-02 16:04:57.950607: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2b00 of size 256 next 99\n",
      "2022-03-02 16:04:57.950615: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2c00 of size 256 next 100\n",
      "2022-03-02 16:04:57.950622: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2d00 of size 256 next 101\n",
      "2022-03-02 16:04:57.950629: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13031a2e00 of size 256 next 102\n",
      "2022-03-02 16:04:57.950637: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 13031a2f00 of size 6482432 next 34\n",
      "2022-03-02 16:04:57.950646: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13037d1900 of size 2359296 next 25\n",
      "2022-03-02 16:04:57.950654: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1303a11900 of size 2359296 next 38\n",
      "2022-03-02 16:04:57.950661: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1303c51900 of size 9437184 next 32\n",
      "2022-03-02 16:04:57.950669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1304551900 of size 9437184 next 44\n",
      "2022-03-02 16:04:57.950676: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1304e51900 of size 9437184 next 43\n",
      "2022-03-02 16:04:57.950684: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1305751900 of size 9437184 next 45\n",
      "2022-03-02 16:04:57.950691: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1306051900 of size 9437184 next 47\n",
      "2022-03-02 16:04:57.950699: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1306951900 of size 12441600 next 65\n",
      "2022-03-02 16:04:57.950706: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 130752f100 of size 12441600 next 66\n",
      "2022-03-02 16:04:57.950714: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 130810c900 of size 12441600 next 67\n",
      "2022-03-02 16:04:57.950721: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 1308cea100 of size 12441600 next 86\n",
      "2022-03-02 16:04:57.950729: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 13098c7900 of size 12441600 next 87\n",
      "2022-03-02 16:04:57.950738: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 130a4a5100 of size 16363264 next 18446744073709551615\n",
      "2022-03-02 16:04:57.950746: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2022-03-02 16:04:57.950758: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 71 Chunks of size 256 totalling 17.8KiB\n",
      "2022-03-02 16:04:57.950767: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 512 totalling 1.0KiB\n",
      "2022-03-02 16:04:57.950775: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 3 Chunks of size 1024 totalling 3.0KiB\n",
      "2022-03-02 16:04:57.950784: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2022-03-02 16:04:57.950793: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 2048 totalling 10.0KiB\n",
      "2022-03-02 16:04:57.950802: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 3072 totalling 3.0KiB\n",
      "2022-03-02 16:04:57.950811: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 6912 totalling 6.8KiB\n",
      "2022-03-02 16:04:57.950820: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 279552 totalling 273.0KiB\n",
      "2022-03-02 16:04:57.950829: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 442368 totalling 432.0KiB\n",
      "2022-03-02 16:04:57.950838: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 589824 totalling 576.0KiB\n",
      "2022-03-02 16:04:57.950846: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 2064384 totalling 1.97MiB\n",
      "2022-03-02 16:04:57.950855: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 2359296 totalling 4.50MiB\n",
      "2022-03-02 16:04:57.950864: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 4718592 totalling 4.50MiB\n",
      "2022-03-02 16:04:57.950873: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 9437184 totalling 45.00MiB\n",
      "2022-03-02 16:04:57.950882: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 5 Chunks of size 12441600 totalling 59.33MiB\n",
      "2022-03-02 16:04:57.950891: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 16363264 totalling 15.60MiB\n",
      "2022-03-02 16:04:57.950900: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 132.19MiB\n",
      "2022-03-02 16:04:57.950908: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 145096704 memory_limit_: 145096704 available bytes: 0 curr_region_allocation_bytes_: 290193408\n",
      "2022-03-02 16:04:57.950920: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                       145096704\n",
      "InUse:                       138614272\n",
      "MaxInUse:                    138614272\n",
      "NumAllocs:                         443\n",
      "MaxAllocSize:                 16363264\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-03-02 16:04:57.950937: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ******____****************************************************************************************xx\n",
      "2022-03-02 16:04:57.950974: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES faile"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[12800,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11329/20347676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# note the use of the Flatten layer before passing the features to a Dense layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py39-tf2/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py39-tf2/lib/python3.9/site-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._generator.uniform(\n\u001b[1;32m   1919\u001b[0m           shape=shape, minval=minval, maxval=maxval, dtype=dtype)\n\u001b[0;32m-> 1920\u001b[0;31m     return tf.random.uniform(\n\u001b[0m\u001b[1;32m   1921\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m         seed=self.make_legacy_seed())\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12800,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d at random_op.cc:74 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[12800,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(5, 5, 512))\n",
    "\n",
    "# note the use of the Flatten layer before passing the features to a Dense layer\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:47:12.472190Z",
     "start_time": "2020-03-24T19:46:25.232894Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"../models/feature_extraction.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=30,\n",
    "                    validation_data=(val_features, val_labels),\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training should be very fast, even on a cpu.  Lets look\n",
    "at the accuracy and loss curves during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:47:31.835395Z",
     "start_time": "2020-03-24T19:47:31.822104Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:47:34.159174Z",
     "start_time": "2020-03-24T19:47:33.343542Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:47:44.521852Z",
     "start_time": "2020-03-24T19:47:43.814412Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should reach a validation accuracy of about 90%, which\n",
    "is an improvement over our previous network with\n",
    "data augmentation.  However, you should see that the model\n",
    "overfits almost immediately, even using a fairly large\n",
    "dropout layer. You can tell that it is overfitting because\n",
    "if you look at the loss curves, loss continues to decrease\n",
    "on the training data set during training, but validation\n",
    "loss quickly stops decreasing and starts increasing again\n",
    "after 3 or 4 epochs.  Overfitting is still a big problem\n",
    "with this dataset because our image dataset is so small.\n",
    "So this implies we can do even better if we try the pretrained\n",
    "network together with data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction with data augmentation\n",
    "\n",
    "Now we will try the second technique, combining the pretrained\n",
    "network with new densely connected layers, and training it\n",
    "with a augmented data stream of images.  This will\n",
    "be much slower that the previous approach of extracting\n",
    "the feature maps statically, so you may need a gpu system\n",
    "to even attempt the following training realistically.\n",
    "\n",
    "Because models behave just like layers in `keras`, you can\n",
    "add a model (like conv_base) to a `Sequential` model just\n",
    "like you would add a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "conv_base.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "# apply data augmentation\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "# apply input value scaling\n",
    "x = keras.applications.vgg16.preprocess_input(x)\n",
    "\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:48:15.045183Z",
     "start_time": "2020-03-24T19:48:15.020765Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T19:48:10.200422Z",
     "start_time": "2020-03-24T19:48:09.961743Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "            optimizer=\"rmsprop\",\n",
    "            metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the summary of the model that we created by smashing\n",
    "together the VGG16 convolution layers with our new\n",
    "fully connected dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the convolutional base of the VGG16 model\n",
    "has over 14 million parameters, which is pretty large.  The\n",
    "classifier we added on top has over 2 million parameters.\n",
    "\n",
    "As mentioned before, we don't actually want to train the\n",
    "VGG16 layer weights, we want them to stay fixed.  Thus\n",
    "we need to *freeze* the convolutional base layers.  Freezing\n",
    "a layer or set of layers means preventing their weights\n",
    "from being updated during training.  If we don't do this then\n",
    "the representations that were previously learned by the\n",
    "VGG16 convolutional base will be modified during training.\n",
    "\n",
    "In `keras` we freeze a network by setting its `trainable`\n",
    "attribute to `False`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training our densely connected layers in our\n",
    "model with the same data augmentation configuration we used\n",
    "before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:27:19.355675Z",
     "start_time": "2020-03-24T19:49:03.156075Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "            filepath=\"../models/feature_extraction_with_data_augmentation.keras\",\n",
    "            save_best_only=True,\n",
    "            monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=calbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the accuracy and loss curves again to determine\n",
    "how the training went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:27:42.785702Z",
     "start_time": "2020-03-24T20:27:42.774976Z"
    }
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "\n",
    "val_acc = history.history['val_accccuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:27:46.410557Z",
     "start_time": "2020-03-24T20:27:45.652505Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:27:56.093656Z",
     "start_time": "2020-03-24T20:27:55.360360Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that accuracy improves, probably to around\n",
    "96%.  And as before, the data augmentation prevents\n",
    "overfitting as can be seen from the validation loss curve\n",
    "which mostly follows the training loss.\n",
    "\n",
    "Lets check the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model(\n",
    "    \"../models/feature_extraction_with_data_augmentation.keras\")\n",
    "\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3.2 Fine-tuning\n",
    "\n",
    "Another widely used technique with reuse of pretrained\n",
    "networks, complementary to fixed feature extraction like\n",
    "we did in both cases above, is to allow for fine-tuning of\n",
    "some of the pretrained convolutional layers instead.  So\n",
    "in this technique, instead of freezing all of the\n",
    "layers of the pretrained convolutional base, we might leave\n",
    "1 or a few of the highest convolutional layers unfrozen,\n",
    "so that they can be modified by training.  The idea here is,\n",
    "in convolutional layers, the higher we go the more abstract the features.  So for a new image classification task, there might be use in using convolution layers to better fit or\n",
    "learn these high level features of the particular task.\n",
    "\n",
    "However, because of a tendency for large errors to propogate,\n",
    "it is unwise to train both the new random fully connected\n",
    "layers at the same time we are fine-tuning existing\n",
    "convolutional layers.  Thus the steps we will follow when\n",
    "fine-tuning the network are\n",
    "\n",
    "1. Add our custom fully connected network on top of an\n",
    "   already trained base network.\n",
    "2. Freeze the base network.\n",
    "3. Train the part we added.\n",
    "4. Unfreeze some layers in the base network.\n",
    "5. Jointly train both these layers and the part we added.\n",
    "\n",
    "Above we already completed the first 3 steps.  So we\n",
    "can try fine tuning by now unfreezing some of the layers\n",
    "in the convolutional base and then continue to do some\n",
    "more training.\n",
    "\n",
    "We will unfreeze and fine tune the last 3 convolutional \n",
    "layers, which means we want to unfreeze the block5\n",
    "convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:28:15.193565Z",
     "start_time": "2020-03-24T20:28:15.181457Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-24T20:28:18.295533Z",
     "start_time": "2020-03-24T20:28:18.271861Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can begin fine-tuning the network.  We will do this\n",
    "using the `RMSProp` optimizer with a very low learning\n",
    "rate.  The reason for a low learning rate is that\n",
    "we want to limit the magnitude of the modifications\n",
    "we make to the representations of the three layers we are\n",
    "fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as mentioned in text, you need to make sure you (re)compile\n",
    "# model after freezing or unfreezing layers so that those\n",
    "# settings take effect\n",
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as usual we will plot loss and accuracy curves of this\n",
    "most recent training to determine how the training went."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accurach']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
